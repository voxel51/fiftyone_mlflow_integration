{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #ff6D04; \">MLflow + FiftyOne Workflow</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Walkthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "First install the required python libraries below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow fiftyone torch torchvision voxel51-eta[storage] ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Next we will install the fiftyone-mlflow-plugin that will allow us to view and manage our MLflow client in the FiftyOne App! The App can be run in your browser at localhost:5151 or even in your Databricks Notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/fiftyone_mlflow_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the MLflow Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will start our MLflow server locally to serve as our backend for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter the following in another bash terminal in the same project directory!\n",
    "\n",
    "!mlflow server --backend-store-uri runs/mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick things off by loading in all of our required libraries. While we are at it, we will start our MLflow client and specifying our `tracking_uri`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import json_util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set tracking URI across libraries\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\"\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.operators as foo\n",
    "import fiftyone.plugins as fop\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.utils.random as four\n",
    "\n",
    "from fiftyone import ViewField as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example workflow, I will be using a subset of the [VisDrone](https://github.com/VisDrone/VisDrone-Dataset?tab=readme-ov-file) dataset, a state of the art drone imagery dataset from  Lab of Machine Learning and Data Mining, Tianjin University, China. It features a wide range of locations, time of day, objects, and angles. The subset we will be using can be downloaded on [Google Drive](https://drive.google.com/file/d/1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn/view). Once the file is downloaded and unzipped, we can load it in by following our ingestor below!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!eta gdrive download --public 1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn VisDrone2019-DET-train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip VisDrone2019-DET-train.zip -d VisDrone-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [469.6ms elapsed, 0s remaining, 13.8K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [971.6ms elapsed, 0s remaining, 6.7K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dir=\"./VisDrone-train/VisDrone2019-DET-train/images\"\n",
    "name = \"VisDrone\"\n",
    "\n",
    "# Create the dataset by loading in the directory of images\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    name=name,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# We compute the metadata of the dataset to get height and width of all our samples\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VisDrone features 12 different classes which we will create a dictionary for. The annotations are stored as <x, y, w, h, confidence, label, truncation, occlusion> in txt files. Since it is a custom format, we ingest it by looping through our datasets and grabbing each sample. Next we open up the text file and add the detections and all their metadata on a sample by sample basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {0:\"ignore_regions\",\n",
    "             1:\"pedestrians\",\n",
    "             2:\"people\",\n",
    "             3:\"bicycle\",\n",
    "             4:\"car\",\n",
    "             5:\"van\",\n",
    "             6:\"truck\",\n",
    "             7:\"tricycle\",\n",
    "             8:\"awning_tricycle\",\n",
    "             9:\"bus\",\n",
    "             10:\"motor\",\n",
    "             11:\"others\",\n",
    "}\n",
    "\n",
    "ann_dir = \"./VisDrone-train/VisDrone2019-DET-train/annotations/\"\n",
    "\n",
    "for sample in dataset:\n",
    "\n",
    "    # Grab the annotation file\n",
    "    filename = os.path.basename(sample.filepath)\n",
    "    ann = ann_dir + os.path.splitext(filename)[0] + \".txt\"\n",
    "    if os.path.exists(ann):\n",
    "        with open(ann, 'r') as file:\n",
    "            detections = []\n",
    "            for line in file:\n",
    "                split_line = line.strip().split(\",\")\n",
    "                ann_list = [int(x) for x in split_line[:8]]\n",
    "\n",
    "                # Grab all the detection information from the line\n",
    "                label = class_map[ann_list[5]]\n",
    "                trunc = ann_list[6]\n",
    "                occ = ann_list[7]\n",
    "\n",
    "                # FiftyOne takes in normalized (x,y,w,h) bounding boxes\n",
    "                x = ann_list[0] / sample.metadata.width\n",
    "                y = ann_list[1] / sample.metadata.height\n",
    "                w = ann_list[2] / sample.metadata.width\n",
    "                h = ann_list[3] / sample.metadata.height\n",
    "                det = fo.Detection(\n",
    "                    label=label,\n",
    "                    bounding_box = [x,y,w,h],\n",
    "                    truncation=trunc,\n",
    "                    occlusion=occ\n",
    "                )\n",
    "                detections.append(det)\n",
    "\n",
    "            sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "            sample.save()\n",
    "\n",
    "# Set our dataset as persistent\n",
    "dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading both our images and annotations in, we set the dataset as persistent to have it persist in the database and make sure any new changes will saved. This also allows for easy reloading on future sessions with the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset(\"VisDrone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can launch our FiftyOne app with the line below to visualize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.open('http://localhost:5151/');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dan/anaconda3/envs/dev/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/dan/anaconda3/envs/dev/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/dan/anaconda3/envs/dev/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dan/Documents/fiftyone/fiftyone/core/session/client.py\", line 120, in run_client\n",
      "    raise e\n",
      "  File \"/home/dan/Documents/fiftyone/fiftyone/core/session/client.py\", line 117, in run_client\n",
      "    subscribe()\n",
      "  File \"/home/dan/Documents/fiftyone/fiftyone/core/session/client.py\", line 110, in subscribe\n",
      "    event = Event.from_data(message.event, message.data)\n",
      "  File \"/home/dan/Documents/fiftyone/fiftyone/core/session/events.py\", line 67, in from_data\n",
      "    data[\"state\"] = fos.StateDescription.from_dict(data[\"state\"])\n",
      "  File \"/home/dan/Documents/fiftyone/fiftyone/core/state.py\", line 185, in from_dict\n",
      "    spaces = Space.from_dict(json_util.loads(spaces))\n",
      "  File \"/home/dan/anaconda3/envs/dev/lib/python3.9/site-packages/bson/json_util.py\", line 503, in loads\n",
      "    return json.loads(s, *args, **kwargs)\n",
      "  File \"/home/dan/anaconda3/envs/dev/lib/python3.9/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not dict\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can begin the data curation process and begin to look for issues or mistakes in our datasets. We can leverage powerful features within FiftyOne to help bring new insights into our dataset and create high quality subsets of our data to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Visualize embeddings with FiftyOne Brain](https://docs.voxel51.com/user_guide/brain.html#visualizing-embeddings)\n",
    "- [Search your datasets with text prompts or sort by similarity](https://docs.voxel51.com/user_guide/brain.html#similarity)\n",
    "- [Find image quality issues](https://github.com/jacobmarks/image-quality-issues)\n",
    "- [Find exact and approximate duplicates](https://github.com/jacobmarks/image-deduplication-plugin)\n",
    "- [Find outliers in your dataset](https://github.com/danielgural/outlier_detection)\n",
    "- [Create interesting views of your dataset by filtering, slicing, sorting, and more!](https://docs.voxel51.com/user_guide/using_views.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these curation tools, the MLFlow panel and more are powered by [FiftyOne Plugins](https://github.com/voxel51/fiftyone-plugins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created a view you like, we need to export the dataset in YOLO format in order to train YOLO9. We do so by randomly splitting and using the `export` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1779/1779 [12.3s elapsed, 0s remaining, 126.8 samples/s]      \n",
      "Directory 'VisDrone_curated/' already exists; export will be merged with existing files\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6308/6308 [43.9s elapsed, 0s remaining, 91.8 samples/s]       \n",
      "Directory 'VisDrone_curated/' already exists; export will be merged with existing files\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0/0 [6.2ms elapsed, ? remaining, ? samples/s] \n"
     ]
    }
   ],
   "source": [
    "class_map = {0:\"ignore_regions\",\n",
    "             1:\"pedestrians\",\n",
    "             2:\"people\",\n",
    "             3:\"bicycle\",\n",
    "             4:\"car\",\n",
    "             5:\"van\",\n",
    "             6:\"truck\",\n",
    "             7:\"tricycle\",\n",
    "             8:\"awning_tricycle\",\n",
    "             9:\"bus\",\n",
    "             10:\"motor\",\n",
    "             11:\"others\",\n",
    "}\n",
    "\n",
    "# Replace below with you own saved view, or use the whole dataset\n",
    "#curated = dataset.load_saved_view(\"Curated\")\n",
    "curated = dataset\n",
    "\n",
    "four.random_split(curated, {\"val\": 0.15, \"train\": 0.85})\n",
    "classes = list(class_map.values())\n",
    "\n",
    "for split in [\"val\",\"train\",\"test\"]:\n",
    "    view =  curated.match_tags(split)\n",
    "    view.export(\n",
    "        export_dir=\"VisDrone_curated/\",\n",
    "        split=split,\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        classes=classes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will be training with Ultralytics YOLOv9. We will take advantage of the Ultralytics MLflow integration to round out our stack for this workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run will be stored on MLFlow with information of the hyperparameters, dataset contents, and metrics during training like mAP score! A custom run will also be saved to the FiftyOne dataset that saves information like the tracking_uri and experiment name from MLFlow as well as allows for you to come back to the view the run was trained on whenever! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9c summary: 618 layers, 25590912 parameters, 0 gradients, 104.0 GFLOPs\n",
      "New https://pypi.org/project/ultralytics/8.1.28 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.25 üöÄ Python-3.9.18 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12010MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov9c.pt, data=../VisDrone_curated/dataset.yaml, epochs=3, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=mlflow_fiftyone, name=Curated, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=mlflow_fiftyone/Curated\n",
      "Overriding model.yaml nc=80 with nc=12\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    212864  ultralytics.nn.modules.block.RepNCSPELAN4    [128, 256, 128, 64, 1]        \n",
      "  3                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      "  4                  -1  1    847616  ultralytics.nn.modules.block.RepNCSPELAN4    [256, 512, 256, 128, 1]       \n",
      "  5                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  6                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  7                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  8                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPELAN         [512, 512, 256]               \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    912640  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 256, 256, 128, 1]      \n",
      " 16                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1   2988544  ultralytics.nn.modules.block.RepNCSPELAN4    [768, 512, 512, 256, 1]       \n",
      " 19                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 22        [15, 18, 21]  1   5592052  ultralytics.nn.modules.head.Detect           [12, [256, 512, 512]]         \n",
      "YOLOv9c summary: 618 layers, 25538484 parameters, 25538468 gradients, 103.7 GFLOPs\n",
      "\n",
      "Transferred 931/937 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir mlflow_fiftyone/Curated', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/dan/Documents/databricks/VisDrone_curated/labels/train.cac\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/train/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/train/0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/train/9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/train/9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/train/9999998_00219_d_0000175.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/dan/Documents/databricks/VisDrone_curated/labels/val.cache..\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /home/dan/Documents/databricks/VisDrone_curated/images/val/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to mlflow_fiftyone/Curated/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000625, momentum=0.9) with parameter groups 154 weight(decay=0.0), 161 weight(decay=0.0005), 160 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 08:43:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2024/03/15 08:43:52 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of transformers. If you encounter errors during autologging, try upgrading / downgrading transformers to a supported version, or try upgrading MLflow.\n",
      "2024/03/15 08:43:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(2de95c995b474d2fb45a8a6ee06a2400) to http://127.0.0.1:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmlflow_fiftyone/Curated\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3      4.42G      1.428      1.335     0.9815        320        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.422      0.268      0.256       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      4.46G      1.371      1.085     0.9581        193        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.471      0.293      0.287      0.167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      4.99G      1.314     0.9951     0.9423        405        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.443      0.328      0.324      0.193\n",
      "\n",
      "3 epochs completed in 0.135 hours.\n",
      "Optimizer stripped from mlflow_fiftyone/Curated/weights/last.pt, 51.6MB\n",
      "Optimizer stripped from mlflow_fiftyone/Curated/weights/best.pt, 51.6MB\n",
      "\n",
      "Validating mlflow_fiftyone/Curated/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.25 üöÄ Python-3.9.18 torch-2.2.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 Laptop GPU, 12010MiB)\n",
      "YOLOv9c summary (fused): 384 layers, 25328500 parameters, 0 gradients, 102.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1779      97275      0.443      0.327      0.323      0.192\n",
      "        ignore_regions       1779       2342      0.298    0.00171     0.0177     0.0055\n",
      "           pedestrians       1779      20413      0.571      0.356      0.398      0.181\n",
      "                people       1779       7698      0.503       0.12      0.164     0.0553\n",
      "               bicycle       1779       2871      0.294      0.164      0.138     0.0588\n",
      "                   car       1779      40695      0.627      0.709      0.706      0.463\n",
      "                   van       1779       6980      0.536      0.459      0.474      0.327\n",
      "                 truck       1779       3429      0.548      0.506      0.504      0.345\n",
      "              tricycle       1779       1236      0.336      0.279      0.212      0.122\n",
      "       awning_tricycle       1779        835       0.33       0.29      0.227      0.137\n",
      "                   bus       1779       1667      0.544      0.632      0.614      0.438\n",
      "                 motor       1779       8653      0.474      0.347      0.337      0.142\n",
      "                others       1779        456      0.254     0.0614     0.0823     0.0351\n",
      "Speed: 0.1ms preprocess, 3.3ms inference, 0.0ms loss, 4.7ms postprocess per image\n",
      "Results saved to \u001b[1mmlflow_fiftyone/Curated\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to http://127.0.0.1:5000\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [4.0m elapsed, 0s remaining, 17.9 samples/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-5' coro=<execute_or_delegate_operator() running at /home/dan/Documents/fiftyone/fiftyone/operators/decorators.py:22>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "log_mlflow_run = foo.get_operator(\"@jacobmarks/mlflow_tracking/log_mlflow_run\")\n",
    "\n",
    "\n",
    "# Build a YOLOv9c model from pretrained weight\n",
    "model = YOLO('yolov9c.pt')\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Train the model on the  dataset for 1 epochs, set project name for experiment_name, name for the run name\n",
    "model.train(\n",
    "    data='../VisDrone_curated/dataset.yaml',\n",
    "    epochs=3,\n",
    "    imgsz=640, \n",
    "    batch=4,\n",
    "    project=\"mlflow_fiftyone\",\n",
    "    name=\"Curated\"\n",
    ")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"Curated\")\n",
    "\n",
    "\n",
    "#Log the completed run to our FiftyOne Dataset\n",
    "log_mlflow_run(\n",
    "        dataset, \"mlflow_fiftyone\", predictions_field=\"Curated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our run, we can monitor its status in the FiftyOne App through the MLFlow panel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/mlflow.gif\" alt=\"MLFLow Monitoring\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even come back to a run whenever using the `show_mlflow_run` operator! With it, we can select an experiment followed by a run to pull up the panel to see training results as well as all the samples it was trained on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/view_mlflow_run.gif\" alt=\"MLFLow Run\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Predictions to Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin adding predictions to our dataset, we load in our model from a checkpoint with the code below if needed. If the model is still loaded after training we can continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO('mlflow_fiftyone/curated/weights/best.pt')  # load from training run if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can just pass our Ultralytics YOLOv9 model to `apply_model` to add detections to all of our samples!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||--------------|    0/6471 [12.1ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [4.0m elapsed, 0s remaining, 17.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(model, label_field=\"predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's view our dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.dataset = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `evaluate_detections` and calculate the mAP of our model. We also add metadata to our sample detections such if they were a false potive or a true positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [20.0m elapsed, 0s remaining, 6.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6471/6471 [6.8m elapsed, 0s remaining, 14.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "results = dataset.evaluate_detections(pred_field=\"predictions\", gt_field=\"ground_truth\", eval_key=\"eval\", compute_mAP=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the workflow of adding predictions and evaluating for any number of models on our dataset! You can even compare predicitions from one model to another using the [model comparision](https://github.com/allenleetc/model-comparison) plugin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/model_compare_input.gif\" alt=\"Model Compare Input\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose from a variety of options to see exactly where your two models differ. Forget searching across hundreds of thousands of detection, the model comparision plugin will bring only the samples of interest right in front of you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/model_compare_out.gif\" alt=\"Model Compare Input\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained model can also help use during data curation! One of the most common ways is to check your high confidence false postives. This is where you are most likely to find annotation mistakes in your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/high_cf_fp.gif\" alt=\"High Conf False Positives\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
